---
title: "HW4_jaeyounglee"
author: "Jaeyoung Lee"
date: \today
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(data.table)
library(doParallel)
library(parallel)
library(ggplot2)
library(ggpubr)
```

## Problem 2

Given $\mathbf{X}$ and $\vec{h}$ below, implement the above algorithm and compare the results with lm(h~0+$\mathbf{X}$).  State the tolerance used and the step size, $\alpha$.

```{r gradient}
# Random seed and true values
set.seed(1256)
theta <- as.matrix(c(1,2),nrow=2) # True parameter (theta_0, theta_1)
X <- cbind(1,rep(1:10,10))        # Design matrix
h <- X%*%theta+rnorm(100,0,0.2)   # True y value

# Parameters for the algorithm
tolerance <- 1e-9                 # Tolerance
alpha <- 1e-7                     # Step size
result <- NULL                    # Collect interesting information

# Set initial values
initial_value <-c(runif(1,0,2), runif(1,1,3)) # Initial value of theta
theta_new <- initial_value                    # Initial value of theta
theta_old <- c(10,10)                         # Initial value to operate loop
no_iter <- 1                                  # Number of iterations
  
# Gradient Descent algorithm
# If both theta values are smaller than tolerance, than it converges
while(all(abs(theta_new - theta_old) > tolerance) & no_iter < 5e+6 ){
  # Update new x value
  theta_old <- theta_new
  
  # Gradient Descent formula
  theta_new[1] <- theta_old[1] - alpha/length(h) * sum(X%*%theta_old - h) 
  theta_new[2] <- theta_old[2] - alpha/length(h) * sum((X%*%theta_old - h)*X[,2])
  
  # Count the number of iteration
  no_iter <- no_iter + 1
}
# Collect iterations, initial values used, estimated theta
result <- rbind(result, c(no_iter, initial_value, theta_new))
result <- result %>% data.frame
names(result) <- c('no_iter', 'theta_0_start', 'theta_1_start', 
                   'theta_0', 'theta_1')
result

# Comparison with lm function
lmfit <- lm(h~0+X) # Fitting with simple linear regression
lmfit$coefficients
  
```  
The outputs above are result of Gradient Descent and simple linear regression. Tolerance is $1e-9$ and the step size $\alpha$ is $1e-7$. The starting values for the Gradient descent algorithm are generated from uniform random numbers. From the outputs we can notice that the estimates for $\Theta_0$ of two methods are quite different. On the other hand, $\Theta_1$ values are quite similar. We expected that two methods have similar results. The difference might come from initial values and the step size $\alpha$. If we use different initial values and step size, then they will have similar result. Thus, in *Problem 3* we will generate 10000 initial values. 

\pagebreak


## Problem 3

### Part a. Making sure to take advantages of parallel computing opportunities. 

```{r gradient_parallel, eval=FALSE}
# Random seed and true values
set.seed(1256)
theta <- as.matrix(c(1,2),nrow=2) # True parameter (theta_0, theta_1)
X <- cbind(1,rep(1:10,10))        # Design matrix
h <- X%*%theta+rnorm(100,0,0.2)   # True y value
m <- length(h)                    # length of y (To speed up)

# Parameters for the algorithm
tolerance <- 1e-9                 # Tolerance
alpha <- 1e-7                     # Step size

# Generate 10000 initial values
n <- 10000                         # The number of initial value vector
initial_value <- cbind(runif(n,0,2), runif(n,1,3))

# Speed up using parallel computing
cores <- detectCores() - 1 # Use almost all the cores
cl <- makeCluster(cores)   # Create a cluster via makeCluster
registerDoParallel(cl)     # Register the cluster

# Making advantage of parallel computing
# Collect the interesting information from foreach
gradient_result <- foreach(init=1:n, .combine = rbind) %dopar% {
  # Set initial values
  theta_new <- initial_value[init,]  # Initial value of theta
  theta_old <- c(10,10)              # Initial value to operate loop
  no_iter <- 1                       # Number of iterations
  
  # Gradient Descent algorithm
  # If both theta values are smaller than tolerance, than it converges
  while(all(abs(theta_new - theta_old) > tolerance) & no_iter < 5e+6 ){
    # Update new x value
    theta_old <- theta_new
    h0 <- X%*%theta_old
    
    # Gradient Descent formula
    theta_new[1] <- theta_old[1] - alpha/m * sum(h0 - h) 
    theta_new[2] <- theta_old[2] - alpha/m * sum((h0 - h)*X[,2])
    
    # Count the number of iteration
    no_iter <- no_iter + 1
  }
  c(no_iter, initial_value[init,], theta_new)
}

# Stop the cluster
stopCluster(cl)

# Save R Data
save.image(file = 'gradient_descent.RData')

```


```{r parallel_result}
# Load R Data
load(file = 'gradient_descent.RData')

# Make data frame of the result
gradient_result <- gradient_result %>% data.frame()
names(gradient_result) <- c('no_iter', 'theta0_start', 'theta1_start', 'theta0', 'theta1')
head(gradient_result)

# Minimum and maximum number of iteration, their initial values and theta estimates
gradient_result %>% filter(no_iter == min(no_iter)) %>% head()
gradient_result %>% filter(no_iter == max(no_iter)) %>% head()

# Mean of theta estimates
gradient_result %>% select(theta0, theta1) %>% apply(2, mean)  %>% data.frame

# Standard deviation of theta estimates
gradient_result %>% select(theta0, theta1) %>% apply(2, sd) %>%  data.frame




```
From the output, we can know that some initial values do not work well. The minimum number of iteration is $2$ with 25 cases. It means that the algorithm fails to work with the given initial values. The maximum number of iteration is $5M$ with 66 cases. It means than the algorithm did not converge with the given initial values. However, they are extreme cases among $10000$ simulations. From the mean of $10000$ samples, the estimates are similar to the true value $\Theta_0 = 1$, and $\Theta_1 = 2$. Therefore, the algorithm converges well to the true parameters. To compare the standard deviations, the standard deviation of $\Theta_0$ is larger than that of $\Theta_1$.

In addition, using parallel computing, it truly becomes faster.


### Part b. 

When we assume certain true values, we can conduct simulations using the assumed values like the result above. In practical situation, we do not know the true values. This means that it is impossible to put the true value into the stopping rule. However, we can conduct 'Explanatory Data Analysis', so from the result from EDA, we can generate random numbers based on the summary statistics. Then, it will have desirable result.



### Part c. 

The Gradient Descent algorithm we used here can be used as an alternative way of estimating parameters. It is similar to Newton's method. The Gradient Descent is already a popular algorithm, but it requires heavy computation and highly rely on initial values and step size. Therefore, in my opinion, the algorithm is good but need to be careful when we use it.


## Problem 4: Inverting matrices

Ok, so John Cook makes some good points, but if you want to do:

\begin{equation*}
\hat\beta = (X'X)^{-1}X'\underline{y}
\end{equation*}

what are you to do??  Can you explain what is going on?

The above equation is from linear regression. In R, there are `lm` function to find the regression coefficients. However, we can use `solve` function and find the coefficients. Also, instead of using `t` function, `crossprod` function works faster. Therefore, the command `solve(crosprod(x), crossprod(x,y))` would work well.

\pagebreak

## Problem 5: Need for speed challenge

\begin{equation}
y = p + A B^{-1} (q - r)
\end{equation}

Where A, B, p, q and r are formed by:

```{r data, echo=T, eval=F, include=T}

    set.seed(12456) 
    
    G <- matrix(sample(c(0,0.5,1),size=16000,replace=T),ncol=10)
    R <- cor(G) # R: 10 * 10 correlation matrix of G
    C <- kronecker(R, diag(1600)) # C is a 16000 * 16000 block diagonal matrix
    id <- sample(1:16000,size=932,replace=F)
    q <- sample(c(0,0.5,1),size=15068,replace=T) # vector of length 15068
    A <- C[id, -id] # matrix of dimension 932 * 15068
    B <- C[-id, -id] # matrix of dimension 15068 * 15068
    p <- runif(932,0,1)
    r <- runif(15068,0,1)
    C<-NULL #save some memory space
```

Part a.

How large (bytes) are A and B?  Without any optimization tricks, how long does the it take to calculate y?

```{r speedup, eval=FALSE}

    set.seed(12456) 
    
    G <- matrix(sample(c(0,0.5,1),size=16000,replace=T),ncol=10)
    R <- cor(G) # R: 10 * 10 correlation matrix of G
    C <- kronecker(R, diag(1600)) # C is a 16000 * 16000 block diagonal matrix
    id <- sample(1:16000,size=932,replace=F)
    q <- sample(c(0,0.5,1),size=15068,replace=T) # vector of length 15068
    A <- C[id, -id] # matrix of dimension 932 * 15068
    B <- C[-id, -id] # matrix of dimension 15068 * 15068
    p <- runif(932,0,1)
    r <- runif(15068,0,1)
    C<-NULL #save some memory space

    # How big are A and B?
    size_A <- object.size(A)
    size_B <- object.size(B)
    
    # Elapsed time computing y
    computing_time_y <- system.time(p + A%*%solve(B, (q-r)))
    
    # Save R Data
    save.image(file = 'need_for_speed.RData')
    
```

```{r eval_speedup, echo = FALSE}
load(file = 'need_for_speed.RData')    

# How big are A and B?
paste('Size of A = ', size_A, 'Byte') 
paste('Size of B = ', size_B, 'Byte') 
    
# Elapsed time computing y
print('Original computing time')
computing_time_y

```
As we can see, the sizes of A and B are large. Also, the computing time is about nine minutes.  


Part b.

The object sizes of $A$ and $B$ matrices are too large. However, they are have a lot of zero elements. Using this fact, we can reduce the object sizes of them using a package named `Matrix`. Using the package we can make the matrices be sparse and reduce the sizes. Since $B$ matrix is too large, it is hard to make it to be sparse. However, if we partition the matrix, we can make the block matrices to be sparse. It is easy to partition and combine matrices. Therefore, it will take much less time than original computation.


Part c.

```{r need_for_speed}
# Need for speed challenge
library(Matrix) # Using Sparse Matrix
# Load R Data
load(file = 'need_for_speed.RData')

# Speed up by making matrices sparse to reduce the object sizes
computing_time_speedup <- system.time({
  A_sparse <- Matrix(A, sparse = TRUE) # Same matrix but reduce the data size
  #B_sparse <- Matrix(B, sparse = TRUE)

  # Make four block matrices of B and be sparse
  B_sparse_11 <- Matrix(B[1:7534, 1:7534], sparse = TRUE)
  B_sparse_12 <- Matrix(B[1:7534, 7535:15068], sparse = TRUE)
  B_sparse_21 <- Matrix(B[7535:15068, 1:7534], sparse = TRUE)
  B_sparse_22 <- Matrix(B[7535:15068, 7535:15068], sparse = TRUE)

  # Sparse matrix of B
  B_sparse <- rbind(cbind(B_sparse_11, B_sparse_12), cbind(B_sparse_21, B_sparse_22))

  p + A_sparse%*%solve(B_sparse, (q-r)) # Same formula with original one
})

computing_time_y       # original computing time
computing_time_speedup # Speed up computing time


```
From `Matrix` package, there is a function `Matrix`. The function has an argument that make a matrix which has a lot of zeros be sparse, so it can reduce the object size of the matrix. Therefore, using sparse matrices, the computing time is much more faster than the original one.



## Problem 3  

a. 
```{r proportion}
# Define a function that computes the proportion of successes in a vector

prop_success <- function(x, success = 1){
  # The vector x has binary outcomes : 0 = fail, 1 = success
  # Define 'Success' argument what value you want to define as success (Character or numeric)
  x[x == success] <- 1          # Change success to 1
  
  # Proportion of success
  if(mode(x) == 'character'){
    prop <- sum(as.numeric(x[x == '1']))/length(x)  # Change the type of data
  }else{
    prop <- sum(x[x == 1])/length(x)                # Sample proportion
  }
  return(prop)
}

# Example using the function
set.seed(10122020)
bin_outcome <- sample(c('f','s'), size = 100, replace = TRUE) # Binary outcomes 
prop_success(bin_outcome, 's') # Proportion of success

```
The defined function above computes the proportion of successes in a vector. The function is made for any type of vector. The function accept both numeric and character vector with binary outcomes. 


b.  
```{r coins}
# A matrix to simulate 10 flips of a coin with varying degrees of "fairness"
set.seed(12345)
P4b_data <- matrix(rbinom(10, 1, prob = (31:40)/100), nrow = 10, ncol = 10, byrow = FALSE)
colnames(P4b_data) <- c('.31', '.32', '.33', '.34', '.35', '.36', '.37', '.38', '.39', '.40')
P4b_data

```
Above is a pre-defined matrix from the problem to simulate 10 flips of a coin with varying degrees of "fairness"


c.
```{r apply_function}
# Apply function with the custum function
apply(P4b_data, 2, prop_success)

```
It seems working well. The function brings correct result.


d. 
```{r prob_to_flip}
# Coinflip function based on given probabilities
coinflip <- function(p, n = 10){
  # The input n is the number of flips and p is probability
  flips <- sample(c(0,1), size = n, prob = c(1-p, p), replace = TRUE)
  return(flips)
}
# Apply the function on a probability vector using sapply
sapply((31:40)/100,  coinflip)

```
The newly defined function is coin flipping function based on given probabilities. Using `sapply`
 function, we can easily make matrix with the custom function. The matrix is an output of `sapply` and the custom function.

\pagebreak

## Problem 4
  
```{r dev_data}
# Load data
# Multiple repeated measurements from two devices (dev1 and dev2) by thirteen Observers. 
devices <- readRDS('HW3_data.rds')
names(devices) <- c('Observer', 'x', 'y')

# A function of data frame
scatter <- function(devices, observer = 1, col1 = 'brown', col2 = 'black', 
                    main = 'Scatter plot of X and Y'){
  # The inputs are data, observer #, title of plot, and colors
  # Choose colors of single plot and a plot of observer
  # We can choose a scatter plot of certain observer we want to see
  
  # This function is Based on ggplot2, tidyverse, ggpubr package
  require(ggplot2)
  require(tidyverse)
  require(ggpubr)
  
  # A single scatter plot of the entire dataset
  singleplot <- ggplot(data = devices, aes(x=x, y=y)) + 
    geom_point(col = col1, size = 3, shape = 19) + 
    labs(title = main)
  
  # A separate scatter plot using the apply function
  devices_obs <- devices %>% filter(Observer == observer) # Part of data by observer
  separateplot <- ggplot(data = devices_obs, aes(x=x, y=y)) + 
    geom_point(col = col2, size = 3, shape = 19) +
    labs(title = paste('Scatter plot of X & Y of Obs', observer))
  
  ggarrange(singleplot, separateplot, ncol=2)

}

# Single scatter plot and a scatter plot by observers
scatter(devices, 1, col2 = 1)
scatter(devices, 2, col2 = 2)
scatter(devices, 3, col2 = 3)
scatter(devices, 4, col2 = 4)
scatter(devices, 5, col2 = 5)
scatter(devices, 6, col2 = 6)
scatter(devices, 7, col2 = 7)
scatter(devices, 8, col2 = 8)
scatter(devices, 9, col2 = 9)
scatter(devices, 10, col2 = 10)
scatter(devices, 11, col2 = 11)
scatter(devices, 12, col2 = 12)
scatter(devices, 13, col2 = 13)




```

\pagebreak

## Problem 5

Part a. Get and import a database of US cities and states.  

```{r us}
# Load data
states <- fread(input = "./us_cities_and_states/states.sql",skip = 23,
                sep = "'", sep2 = ",", header = F, select = c(2,4))
cities <- fread(input = "./us_cities_and_states/cities_extended.sql",skip = 23,
                sep = "'", sep2 = ",", header = F, select = c(2,4))

# Remove duplicated data
cities <- cities %>% unique()

head(states) # states data
head(cities) # cities data
```

Part b. Create a summary table of the number of cities included by state.

```{r number_of_cities}
# Join states and cities data
# Exclude Puerto Rico
states_cities <- states %>% inner_join(cities, by = "V4") 
names(states_cities) <- c('region', 'code', 'city')

# Find the number of cities by states
# The number of cities in a state 
count_cities <- states_cities %>% select(region, city) %>% 
  group_by(region) %>% summarise(num_cities = n(), .groups = 'rowwise')

# Table of the number of cities by states
knitr::kable(count_cities, col.names = c('States', 'Cities'))

```


Part c. Create a function that counts the number of occurrences of a letter in a string.  The input to the function should be "letter" and "state_name".  

```{r occur}
# A function that couns the number of occurrences of a letter in a string
count_letter <- function(letter, state_name){
  # Count the number of occurrences of a letter in a string
  # Split the state name
  split_name <- unlist(strsplit(state_name,''))
  
  # Determine whether to count the big letter
  capital <- LETTERS[letters == letter]
  if(split_name[1] == capital){
    split_name[1] <- letter
  }
  
  # Find the letter from the string and count the occurrences
  counted <- sum(split_name == letter)
  
  return(counted)
}

```

Create a for loop to loop through the state names imported in part a.  Inside the for loop, use an apply family function to iterate across a vector of letters and collect the occurrence count as a vector.

```{r occur_loop}
# Count with for loop
# Collect the counts to the data frame
letter_count <- data.frame(matrix(NA,nrow=nrow(states), ncol=26))
names(letter_count) <- letters # Columns names

# Collect the counts
for(i in 1:nrow(states)){
  letter_count[i,] <- sapply(letters, count_letter, state_name = states$V2[i])
}

# Data frame of Counts
letter_count

```


Part d. Create 2 maps to finalize this. Map 1 should be colored by count of cities on our list within the state. Map 2 should highlight only those states that have more than 3 occurrences of ANY letter in their name.

```{r map1}
# Create a map colored by count of cities within the state
# Reference : https://remiller1450.github.io/s230s19/Intro_maps.html

# Import US map
library(maps)

# US map data
us_map <- map_data('state')

# Change the states names starting with capital letter
# Names behind spaces
divided_states_small <- c('columbia', 'hampshire', 'jersey', 'mexico', 
                          'york', 'carolina', 'dakota', 'island', 'virginia')
divided_states_big <- c('Columbia', 'Hampshire', 'Jersey', 'Mexico', 
                        'York', 'Carolina', 'Dakota', 'Island', 'Virginia')
# Change the names behind spaces starting with capital letter
for (i in 1:length(divided_states_small)){
  us_map$region <- gsub(divided_states_small[i],divided_states_big[i],us_map$region) 
}

# Change the names staring with capital letter
for(i in 1:nrow(states)){
  us_map$region[substring(us_map$region, 2) == substring(states$V2[i], 2)] <-  states$V2[i]
}

# Join the count_cities with us_map data by region
# Exclude Alaska and Hawaii
territory <- us_map %>% inner_join(count_cities, by = 'region')

# Color the map of The United States
ggplot() + 
  geom_polygon(data= territory, 
               aes(x = long, y = lat, group = group, fill = num_cities), 
               color="white", size = 0.2) +
  labs(title = 'The Map of The United States', 
       subtitle = 'Colored by count of cities within the state')
  

    
```


```{r map2}
# Create a map highlight only those states that have more than 3 occurrences of ANY letter in their name
# Find the counts larger than 3
more_than_three <- data.frame(letter_count>3)

# States that letter counts are larger than 3
letter_count_states <- states$V2 %>% data.frame(apply(more_than_three , 1, sum))
names(letter_count_states) <- c('region', 'letter_count')


# Join the counts with us_map data
territory_letter_count <- us_map %>% inner_join(letter_count_states, by = 'region')
# Handle the counts as discrete
territory_letter_count$letter_count <- territory_letter_count$letter_count %>% as.factor

# Highlight only those states that have more than 3 occurrences of ANY letter in their name
ggplot() + 
  geom_polygon(data= territory_letter_count, 
               aes(x = long, y = lat, group = group, fill = letter_count), 
               color="white", size = 0.2) +
  labs(title = 'The Map of The United States', 
       subtitle = 'Highlighted states that have more than 3 counts of any letter in their name') + 
  scale_fill_manual(values=c("gray", "dodgerblue4", "brown2"))
```


\pagebreak

## Problem 2 Bootstrapping

Recall the sensory data from five operators:    
<http://www2.isye.gatech.edu/~jeffwu/wuhamadabook/data/Sensory.dat> 

What we want to do is bootstrap the Sensory data to get non-parametric estimates of the parameters.  We are really only interested in a linear model lm(y~operator).

### Part a.  First, the question asked in the stackexchange was why is the supplied code not working.  This question was actually never answered.  What is the problem with the code?  If you want to duplicate the code to test it, use the quantreg package to get the data.
```{r stack, eval = FALSE}
# From the Stackexchange
#create df from AAPL returns and market returns
df08<-cbind(logapple08,logrm08)
set.seed(666)
Boot_times=1000
sd.boot=rep(0,Boot)
for(i in 1:Boot){
# nonparametric bootstrap
bootdata=df08[sample(nrow(df08), size = 251, replace = TRUE),]
sd.boot[i]= coef(summary(lm(logapple08~logrm08, data = bootdata)))[2,2]
}

```

First of all, the object name `Boot` is not defined. If the problem appears, there must be mistakes using the random seed. Also, the writer did not use the object `Boot_times`.


### Part b. Bootstrap the analysis to get the parameter estimates using 100 bootstrapped samples.  Make sure to use system.time to get total time for the analysis.  You should probably make sure the samples are balanced across operators, ie each sample draws for each operator.
```{r sensory}
# Bootstrap 

######## Sensory data ########
# Getting "https://www2.isye.gatech.edu/~jeffwu/wuhamadabook/data/Sensory.dat"
url_sensory <- "https://www2.isye.gatech.edu/~jeffwu/wuhamadabook/data/Sensory.dat"
sensory_rawdata <- fread(url_sensory, fill = TRUE, skip = 2, data.table = FALSE)
saveRDS(sensory_rawdata, 'sensory_rawdata.RDS')
sensory_rawdata <- readRDS('sensory_rawdata.RDS')
# Sensory data with tidyverse package
# Making matrix which is the same with base R function but using pipes.
matrix_sensory <- sensory_rawdata %>% as.matrix() %>% t() 
na <- is.na(matrix_sensory==TRUE) %>% which()     # Find missing values

# The indexes where Item numbers are in the data
x <- 1
item <- x
for (i in 1:9){
  x <- x+18
  item <- c(item, x)
}

# Remove missing values and Item numbers from the data
Values <- matrix_sensory[-c(na,item)]

# Bind the values with 'Item' and 'Operator' columns
Item <- paste('Item', 1:10) %>% rep(each = 15)   # Item names
Opr <- paste('Opr', 1:5) %>% rep(30)             # Operator names
sensory_data_tidyverse <- data.table(Item, Opr, Values)

# Final tidy data with tidyverse
sensory_data_tidyverse %>% head()


```

```{r boot}
# Regression with bootstrap
sensory <- sensory_data_tidyverse %>% select(Values, Opr) # We only need item (y) and operator (x)
n <- nrow(sensory) # Sample size
no_boot <- 100     # The number of bootstrap samples 

betas <- NULL      # Each boostrap regression coefficients

# Bootstrap regression
for(i in 1:no_boot){
  boot_data <- sensory[sample(n, size = n, replace = TRUE), ]
  lmfit <- lm(Values ~ Opr, data = boot_data)
  betas <- rbind(betas, lmfit$coefficients)
}

beta_hat <- apply(betas, 2, mean) # Bootstrap regression coefficients

# Linear regression for original data
linear_model <- lm(Values ~ Opr, data = sensory)

# Comparison of coefficients
linear_model$coefficients # Linear regression 
beta_hat                  # Boostrap regression

# Elapsed time
time_boot <- system.time({
  betas <- NULL      # Each boostrap regression coefficients
  # Bootstrap regression
  for(i in 1:no_boot){
    boot_data <- sensory[sample(n, size = n, replace = TRUE), ]
    lmfit <- lm(Values ~ Opr, data = boot_data)
    betas <- rbind(betas, lmfit$coefficients)
  }
  beta_hat <- apply(betas, 2, mean) # Bootstrap regression coefficients
})

```
From the output, we can know than the regression coefficients for both methods are quite similar. Therefore, bootstrap works well with linear regression even though it does not assume any distribution.


### Part c. Redo the last problem but run the bootstraps in parallel (`cl <- makeCluster(8)`), don't forget to `stopCluster(cl)`).  Why can you do this?  Make sure to use system.time to get total time for the analysis.

Create a single table summarizing the results and timing from part a and b.  What are your thoughts?


```{r boot_parallel}
# Bootstrap Regression with parallel computing
# Make cluster of cores
cores <- 8
cl <- makeCluster(cores)
registerDoParallel(cl)     

# Making advantage of parallel computing
# Collect the interesting information from foreach
boot_parallel <- foreach(i=1:n, .combine = rbind) %dopar% {
  boot_data <- sensory[sample(n, size = n, replace = TRUE), ]
  lmfit <- lm(Values ~ Opr, data = boot_data)
  lmfit$coefficients
}
# Stop the cluster
stopCluster(cl)


beta_hat_parallel <- apply(boot_parallel, 2, mean)


# Linear regression for original data
linear_model <- lm(Values ~ Opr, data = sensory)

# Comparison of coefficients
linear_model$coefficients # Linear regression 
beta_hat                  # Bootstrap regression
beta_hat_parallel         # Bootstrap regression with parallel computing


# Computing time comparison
# Make cluster of cores
cores <- 8
cl <- makeCluster(cores)
registerDoParallel(cl)     

# Elapsed time
time_boot_parallel <- system.time({
  # Making advantage of parallel computing
  # Collect the interesting information from foreach
  boot_parallel <- foreach(i=1:n, .combine = rbind) %dopar% {
    boot_data <- sensory[sample(n, size = n, replace = TRUE), ]
    lmfit <- lm(Values ~ Opr, data = boot_data)
    lmfit$coefficients
  }
})
# Stop the cluster
  stopCluster(cl)

# Compare computing time
time_boot
time_boot_parallel


```
From the result, we can know that all three methods have quite similar regression coefficients. This means that bootstrap works well for original method and with parallel computing. To compare the elapsed time, it seems that computing times are also similar for both methods. There are several reasons. First of all, the sample size is not big. Also, the number of bootstrap samples are also not big. If both sample size and the number of bootstrap samples are large enough, then the parallel computing shows much faster time than the original method.

\pagebreak

## Problem 3

Newton's method gives an answer for a root.  To find multiple roots, you need to try different starting values.  There is no guarantee for what start will give a specific root, so you simply need to try multiple.  From the plot of the function in HW4, problem 8, how many roots are there?

Create a vector (`length.out=1000`) as a "grid" covering all the roots and extending +/-1 to either end.  

### Part a.  Using one of the apply functions, find the roots noting the time it takes to run the apply function.

My code from the previous homework and revised for this homework.
```{r newton}
# f(x) = 3^x - sin(x) + cos(5*x)
f <- function(x){
  value <- 3^x - sin(x) + cos(5*x)
  return(value)
}

# f'(x) = 3^x*log(3) - cos(x) - 5*sin(5*x)
f_prime <- function(x){
  value <- 3^x*log(3) - cos(x) - 5*sin(5*x)
  return(value)
}


# Define the function to run Newton's method
find_sol_newton <- function(x, interesting = f, deriv_fun = f_prime){
  # Input x is the initial value to begin the algorithm, t is tolerance
  # Initial values
  x_new <- x     # Initial value to operate Newton's method
  x_old <- 100 # Initial value to operate while loop
  no_iter <- 1   # Number of iteration of the loop
  
  # Newton's Method
  # When x is a vector, break the loop when FALSE for all values in x 
  while(all(abs(x_new-x_old) > 0.0001) & no_iter < 100000){
  x_old <- x_new                               # Update new x value
  x_new <- x_old - f(x_old)/f_prime(x_old)     # Newton's method formula
  no_iter <- no_iter + 1                       # Count the number of iteration
  }
  return(x_new) # Roots 
}

# Multiple initial values
x0 <- seq(-100, 0, length.out = 1000)

# Find roots 
newton_sol <- sapply(x0, find_sol_newton)
# Round roots at 2nd decimal place and delete replicated solutions
newton_roots <- newton_sol %>% round(digits = 2) %>% unique() %>% sort()
print("The roots"); newton_roots 

# Elapsed time
time_newton <- system.time({
  newton_sol <- sapply(x0, find_sol_newton)
  newton_roots <- newton_sol %>% round(digits = 2) %>% unique() %>% sort()
  print("The roots"); newton_roots 
})

time_newton

# Graph of the function
ggplot(data = data.frame(x = 0), mapping = aes(x=x)) +
  stat_function(fun= f, color = 'brown', size = 1) +
  geom_hline(yintercept = 0) + xlim(-100,1) + ylim(-3,3) +
  labs(title="Graph of the function", x ="x", y = "y")
  
  
```
It seems that the function has infinite number of roots. If we limit the supports of x $x \in \{-100, 0\}$, then we have $161$ roots. The number of roots will increase if we wider the supports of x and initial values. The time elapsed when using `sapply` is 0.02

\pagebreak

### Part b.  Repeat the apply command using the equivelant parApply command.  Use 8 workers.  `cl <- makeCluster(8)`.
```{r parapply}
# Make cluster of cores
cores <- 8
cl <- makeCluster(cores)
clusterExport(cl, c('f', 'f_prime'))

# Parallelize
# Find roots 
parallel_sol <- parSapply(cl, -100:0, find_sol_newton, interesting = f, deriv_fun = f_prime)
# Round roots at 2nd decimal place and delete replicated solutions
parallel_roots <- parallel_sol %>% round(digits = 2) %>% unique() %>% sort()
print("The roots"); parallel_roots 


# Elapsed time
time_parallel <- 
system.time({
  parallel_sol <- parSapply(cl, x0, find_sol_newton, interesting = f, deriv_fun = f_prime)
  parallel_roots <- parallel_sol %>% round(digits = 2) %>% unique() %>% sort()
  print("The roots"); parallel_roots
})

time_parallel

# Stop the cluster
stopCluster(cl)

```
It seems that parallel computing is almost similar the original one.


### Create a table summarizing the roots and timing from both parts a and b.  What are your thoughts?

```{r compare_newton}
# Comparison of two methods
# Compare the roots
knitr::kable(cbind(newton_roots, parallel_roots), caption = 'Roots of both method')
knitr::kable(cbind(time_newton, time_parallel), caption = 'Elapsed time')
```

From the table, we can know that the roots for both methods are the same. For the elapsed time, both methods are similar. This is because the original method is quite fast. If we add some computations and conditions for the original algorithm, the difference will be large, so that parallel compution will be faster.

\pagebreak

```{r appendix, ref.label=c(), echo = T, eval=FALSE, include=T}
```
